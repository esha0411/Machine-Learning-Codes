Review of Real-time Visual Driver Distraction
Detection Algorithms

ChristerAhlstrom
christer.ahlstrom@vti.se

Katja Kircher
katja.kircher@vti.se

Swedish National Road and Transport Research Institute (VT1),
581 95 Linkoping, Sweden

ABSTRACT

Many incidents and crashes can be attributed to driver
distraction, and it is essential to learn how to
detectdistraction in order to develop _ efficient
countermeasures. A number of distraction detection
algorithms have been developed over the years, and the
objective of this paper is to summarize available approaches
and to describe these algorithms in a unified framework.The
review is limited to real-time algorithms that are intended to
detect visual distraction.

Author Keywords
Driver distraction, eye tracking, inattention, detection
algorithm.

INTRODUCTION

Driver inattention and distraction are major contributors to
road incidents and crashes [1, 2]. Even so, drivers continue
to engage themselves in distracting tasks such as using their
mobile phones or navigation systems, eating, grooming and
tending to their children. Advanced driver assistance
systems may change this pattern by alerting the driver when
his or her attention diverts from the road.

Driver distraction can be defined as “the diversion of
attention away from activities critical for safe driving
toward a competing activity” [2]. This is a very general
definition where the diversion of attention can be visual,
auditive, physical or cognitive and where the competing
activity can be anything from mobile phone usage to getting
lost in thought. New advances in remote eye tracking
technology provide a means to counteract distracted driving
in real-time. Eye movements can be used to gain access to
several types of distraction. For example, studies have
shown that eye movements are sensitive not only to visual

 

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored.
Abstracting with credit 1s permitted. To copy otherwise, to republish, to
post of servers or redistribute to lists requires prior specific permission
and/or a fee.

MB’10 August 24-27, 2010, Eindhoven, Netherlands
©ACM 2010 ISBN: 978-1-60558-926-8/10/08...$10.00

RIGHTS Li MN K4>p

distraction but also to auditory secondary tasks [3-5].

There are numerous performance indicators that are based
on longitudinal and lateral vehicle control dynamicswhich
correlate with visual as well as cognitive task demands] 68]. These include steering wheel reversal rate, average
proportion of high frequency steering, brake reaction time,
steering entropy, throttle hold, variability in lateral position,
number of lane exceedences, time- and distance headway
and time to collision. Even though many of these
performance indicators seem to be promising secondary
task identifiers, we restrict this survey to gaze based
distraction detection algorithms.The objective of this paper
is thus to summarize available real-time gaze based
approaches for measuring visual driver distraction and to
describe these algorithms in a unified framework. Since the
focus of this review is on real-time assessment of visual
distraction, many after-the-fact methods based on reaction
times, secondary task performance and_ corrective
manoeuvres are left out. A survey of the effects, in contrast
to the prediction, of driver distraction can be found in
Young et al. [9].

PRINCIPLES OF DRIVER DISTRACTION DETECTION

A schematic overview summarizing the structure of most
driver distraction detection algorithms is illustrated in
Figure |. The basis for all algorithms is measures registered
in real-time during driving. They can stem from the driver
(driver behaviour), like eye movements or hand
movements, or they can be logged from the vehicle (driving
behaviour), like speed or lateral position. Furthermore,
situational variables like time and position can be used
(other data). Certain features of these data, like gaze
direction, steering entropy or others are extracted and
possibly fused in order to arrive at a continuous measure of
the driver’s distraction level. This output is then used to
classify the driver’s state of attention. For most algorithms
these states are visually distracted vs. not visually
distracted.

Field Relevant for Driving

Common for all eye or head movement based distraction
algorithms is that they use off-road glances as the basic
source of information. The idea is to define a field relevant
distraction detection algorithm

 

 

 

 

 

 

 

 

 

 

 

 

—
Ss
2
ro 3
oO w
>
‘= oO
sc 2
c
2
w S 2
5 S e 5
Ss oo -= S
a —— _y o ———________ a
iss) = >
oO > — wy —
‘= wo —
£ >o 2 a
&
)
—
ree)
s
Oo
so
— DD
®
<
a)
Oo

 

 

 

 

 

 

 

 

classification

 

 

 

 

warning algorithm

not
bo > distracted

 

no warning

 

 

 

   

-- - Fe FF >
I

| I

|

|

eo warning
Ww

be > © pa
~~
WN

 

 

Figure 1. A schematic overview of the common features of most driver distraction detection algorithms and mitigation systems.

for driving, which is basically the area where the driver is
looking when he or she is driving. If a world model is not
available, the field relevant for driving can, for example, be
defined as a circle [10-12] or a rectangle [13], see Figure 2.
It is also possible to select different shapes. In Kircher et al.
[14], a circle where the lower part was removed was used
so that the dashboard would not be included in the field
relevant for driving. Since there is no information about
where the driver is looking in the real world, the selected
field relevant for driving needs to be positioned in the real
world based on statistics of where the driver has been
looking. This is often done by centering the selected shape
around the largest peak in the distribution of recent gazes.
When enough gaze data has been acquired, it is also
possible to define more than one zone based on the
distribution of the data. For example, Kutila et al. [15] uses
four zones (road ahead, windscreen and left/right exterior
mirror.

 

Figure 2. Examples of three different approaches to select
the field relevant from driving. The leftmost plot show a
circular area, the middle plot shows a rectangular area and
the rightmost plot shows an area that is defined based on the
cockpit of the car.

RIGHTS Li MN K4>p

If the eye tracking systems allows a world model to be
used, the field relevant for driving can be defined based on
different zones related to the interior of the car. This
approach is used by Pohl et al. [16] and Kircher et al. [14],
see Figure 2. In the latter of these two, the field relevant for
driving is defined as the intersection between a viewing
cone of 90 degrees and the vehicle’s windows. This means
that the circular field relevant for driving concept is
expanded with information about the design of the car.

Distraction Estimation

Glances away from the road ahead are usually defined as
glances residing outside the field relevant for driving. The
duration of these glances away from the road ahead is the
basic source of information that all visual distraction
detection algorithms to date are based upon. If the driver is
looking away from the road too often or for too long, the
driver is considered distracted.

The mappings that transform glances away from the road to
a continuous distraction estimate are often very similar. For
example, Zhang et al. [13] used the average duration of
glances away from the road in a 4.3-second wide sliding
window, Donmez et al. [17] used a weighted sum of the
current glance and the average glance duration in a 3second sliding window and Victor [10] used the percentage
of on-road gaze data points in a 60-second sliding window.
A slightly different approach is to use a buffer [14] or a
counter [11] that changes its value when the driver looks
away. Here the counter/buffer reaches a
maximum/minimum value when the driver is judged to be
too distracted.

So far, there has been a direct link from the FRD via the
glance duration to the estimated distraction level in the
sense that all gazes have the same weight, regardless of
where the gaze is directed. However, it is possible to make
this link fuzzier by changing the weight as a function of
where the gaze is directed. One idea is thus to penalize
glances that are far away from the road centre. In the
SafeTE project [12], this was done by the so-called
eccentricity function E(a) = 6.5758—-1/(0.001*a + 0.152).
This is basically a weighting function that favours glances
close to the road centre while penalizing glances with a
large gaze direction angle. The equation is based on a study
by Lamble et al. [18] and is related to visual behaviour and
brake response when a lead vehicle suddenly starts to
decelerate. In cases where a world model is available, it is
possible to use different weights on different objects [14,
16]. For example, the rear view mirrors and_ the
speedometer could have a higher weight as compared to the
field relevant for driving but lower than the middle console
or the glove compartment. Higher weights in this context
mean that the distraction estimate will increase faster while
lower weights have the opposite effect.Other combinations
of the distraction estimation functions mentioned above, 1.e.
glance duration, glance history and eccentricity,has also
been suggested [19].

Distraction Decision

The continuous distraction estimate needs to be mapped to a
decision whether the driver is distracted or not. Basically,
the driver enters the distracted state when a threshold is
reached and returns to the attentive state when some criteria
are fulfilled. The main difference between different
approaches is how to leave the distracted state. One
approach is to require that the driver is looking forward for
some minimum time before he or she is considered to be
attentive [14, 16, 17]. The other approach is that it is
enough for the driver to look back at the road to be
considered fully attentive [11].

Inhibition Criteria

A distraction detection algorithm determines whether a
driver is distracted or not, but when and in which way the
driver will be warned for distraction is determined by a
warning strategy. Information about different warning
strategies is out of the scope of this review. More
information can be found in, for example, Donmez et al.
[20]. However, there are situations when it is not suitable to
give distraction warnings. For instance, if the driver is
braking hard he or she is probably aware of the situation
and should not be disturbed by a warning. For this reason,
certain criteria can be set up to inhibit warnings. Common
criteria include [21]:

e Speed: Below 50 km/h gaze behaviour is not very
uniform. The gaze is often outside the FRD without the

RIGHTS Li MN K4>p

driver being distracted.

e Direction indicators: Changing lanes and turning can
include planned glances outside the FRD.

e Reverse gear: Reverse engaged means that the driver
should look over the shoulder.

e Brake pedal: No warning should be given while driver is
braking, in order not to interfere with critical driving
manoeuvres.

e Steering wheel angle: No warning should be given while
the driver is engaged in substantial changes of direction,
in order not to interfere with critical driving manoeuvres.

e Lateral acceleration: No warning should be given when
the vehicle makes strong movements, in order not to
interfere with critical driving manoeuvres.

IMPROVEMENTS AND FUTURE RESEARCH

Available algorithms for eye tracking based driver
distraction detection attempt to detect visual distraction.
All algorithms can be fitted in a common framework;
determine if the driver is looking at the road or not, convert
this information into a continuous estimate of (visual)
distraction and finally use some rule, often a threshold, to
determine if the estimated level of distraction should be
considered distracted or attentive. The main limitation of
these approaches is that they do not take the current traffic
situation into account. This could be done by allowing the
field relevant for driving to change dynamically over time.
Future research is needed to (a) determine the optimal field
relevant for driving for different traffic situations and traffic
environments and (b) develop technology to be able to
measure the current traffic situation and _ traffic
environment.

Only one of the available algorithms (percent road centre)
was prepared in order to detect internal distraction.
Suggested measures of internal distraction are based on the
concentration of gazes towards the road centre area, which
is higher when the driver is lost in thought. It has been
suggested that other eye movements such as saccades and
microsaccades could be indicative of workload or
inattention. Future research is needed to (a) investigate eye
movement physiology during driving, (b) develop remote
eye tracking technology with higher accuracy so that these
small and fast eye movements can be measured, and (c)
develop algorithms that reliably and accurately detect
different types of eye movements like fixations, saccades
and smooth pursuit from the continuous data stream.

Other distraction indicators such as lateral and longitudinal
control parameters seem to be very task and situation
dependent, and it is questionable whether they can be used
in a general purpose driver distraction detection algorithm.
Future research includes fusion of several data sources,
including situational variables, so that the appropriate set of
performance indicators is used at exactly the right place at
the right time. Even though it might be impossible to
replace eye movement related indicators completely with
driving related parameters, it would be very valuable to be
able to fall back on this type of data when eye tracking is
lost.

REFERENCES

1. Gordon, C.P., Crash Studies of Driver Distraction, in
Driver Distraction: Theory, effect and mitigation, M.A.
Regan, J.D. Lee, and K.L. Young, Editors. CRC Press,
Taylor & Francis Group, London (2009), 281-304.

2. Lee, J.D., K.L. Young, and M.A. Regan, Defining
Driver Distraction, in Driver Distraction: Theory, effect
and mitigation, M.A. Regan, J.D. Lee, and K.L. Young,
Editors. CRC Press, Taylor & Francis Group London
(2009), 31-40.

3. Recarte, M.A. and L.M. Nunes, Effects of verbal and
spatial-imagery tasks on eye fixations while driving.
Journal of Experimental Psychology: Applied, 6,1
(2000), 31-43.

4. Victor, T.W., J.L. Harbluk, and J. Engstrém, Sensitivity
of eye-movement measures to in-vehicle task difficulty.
Transportation Research Part F: Traffic Psychology and
Behaviour, 8, 2 (2005), 167-190.

5. Engstrém, J., E. Johansson, and J. Ostlund, Effects of
visual and cognitive load in real and_ simulated
motorway driving. Transportation Research Part F:
Traffic Psychology and Behaviour, 8, 2 (2005) 97-120.

6. Green, P. and R. Shah, Task time and glance measures
of the use of telematics: A tabular summary of the
literature, in SAfety VEhicles using adaptive Interface
Technology (2004), UMTRI: Ann Arbor.

7. Zylstra, B., et al., Driving performance for dialing,
radio tuning, and destination entry while driving
straight roads. The University of Michigan
Transportation Research Institute: Ann Arbor, MI
(2003).

8. Ostlund, J., et al., Driving performance assessment methods and metrics, in EU Deliverable, Adaptive
Integrated Driver-Vehicle Interface Project (AIDE).
(2005).

9. Young, L.K., M.A. Regan, and J.D. Lee, Measuring the
effects of driver distraction: Direct driving performance
methods and measures, in Driver distraction: Theory,
effects and mitigation, M.A. Regan, J.D. Lee, and K.L.
Young, Editors. CRC PRess London (2009), 85-105.

RIGHTS Li MN K4>p

10. Victor, T.W., Keeping eye and mind on the road, in
Dept of Psychology. Digital Comprehensive Summaries
of Uppsala Dissertations from the Faculty of Social
Sciences 9, Uppsala University: Uppsala (2005).

11. Fletcher, L. and A. Zelinsky. Driver state monitoring to
mitigate distraction. in International Conference on
Driver Distraction. Sydney, Australia(2005).

12. Engstrom, J. and S. Mardh, SafeTE Final Report(2007).

13. Zhang, H., M. Smith, and R. Dufour, A Final Report of
SAfety VEhicles using adaptive Interface Technology
(Phase IT: Task 7C): Visual Distraction (2008).

14. Kircher, K. and C. Ahlstrom, Issues related to the driver
distraction detection algorithm AttenD, in _ Ist
International Conference on Driver Distraction and
Inattention, Gothenburg, Sweden (2009).

15. Kutila, M., et al. Driver Distraction Detection with a
Camera Vision System. in Proceedings of the [EEE
International Conference on Image Processing. San
Antonio, Texas, US (2007).

16.Pohl, J., W. Birk, and L. Westervall, A driverdistraction-based lane-keeping assistance system.
Proceedings of the Institution of Mechanical Engineers
Part I-Journal of Systems and Control Engineering, 221,
14 (2007), 541-552.

17.Donmez, B., L.N. Boyle, and J.D. Lee, Safety
implications of providing real-time feedback to
distracted drivers.Accident Analysis and Prevention, 39,
3 (2007), 581-590.

18. Lamble, D., M. Laakso, and H. Summala, Detection
thresholds in car following situations and peripheral
vision: implications for positioning of visually
demanding in-car displays. Ergonomics, 42, 6, (1999),
807-815.

19. Liang, Y., Detecting Driver Distraction. University of
Iowa (2009).

20.Donmez, B., L.N. Boyle, and J.D. Lee, Designing
feedback to mitigate distraction, in Driver distraction:
Theory, effects and mitigation, M.A. Regan, J.D. Lee,
and K.L. Young, Editors. CRC PRess London (2009),
519-531.

21. Kircher, K., A. Kircher, and F. Claezon, Distraction and
drowsiness. A field study. VTI: Linképing (2009).
